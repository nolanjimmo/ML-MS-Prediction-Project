{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbae6aa9",
   "metadata": {},
   "source": [
    "Notebook for our final project!\n",
    "\n",
    "Team:\n",
    "Nolan Jimmo\n",
    "Nicole Donahue\n",
    "Frederick Carlson\n",
    "Xinyu Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "817b4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports, function def and some file reading\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def conf_matrix_to_df(conf_matrix, target_names):\n",
    "    return pd.DataFrame(conf_matrix, columns=target_names, index=target_names)\n",
    "\n",
    "\n",
    "#reading in EDSS Score data\n",
    "EDSS_FILENAME = \"data/EDSS_Scores.csv\"\n",
    "EDSS_scores = pd.read_csv(EDSS_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48456703",
   "metadata": {},
   "source": [
    "Find the subject ids that have valid EDSS scores to be able to just train model on these subjects data. Storing the valid subject id and scores in a dictionary with the structure: {Subject ID: (baseline score, 6mo score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b631b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sids = {}\n",
    "for i, row in EDSS_scores.iterrows():\n",
    "    if type(row[\"Subject ID \"]) == float:\n",
    "        break\n",
    "    if row[\"EDSS Baseline (Score out of 10) \"] != np.NaN and row[\"EDSS 6mo (Score out of 10) \"] != np.NaN:\n",
    "        valid_sids[(row[\"Subject ID \"])] = (str(row[\"EDSS Baseline (Score out of 10) \"]), str(row[\"EDSS 6mo (Score out of 10) \"]))\n",
    "#print(valid_sids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca0fbe",
   "metadata": {},
   "source": [
    "converting regular EDSS scores to the binary 0, or 1, for low vs. moderate/severe EDSS score. Everything up to 4 will be 0, everything 4 and above will be moderate/severe score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e6ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sids_generalized = {}\n",
    "for key, value in valid_sids.items():\n",
    "    if float(value[0]) < 4:\n",
    "        v1 = 0\n",
    "    else:\n",
    "        v1 = 1\n",
    "    if float(value[1]) < 4:\n",
    "        v2 = 0\n",
    "    else:\n",
    "        v2 = 0\n",
    "    valid_sids_generalized[key] = (v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d6eb6",
   "metadata": {},
   "source": [
    "Get filenames for the valid subject data files out of the data folder, for both the baseline and 6mo data\n",
    "\n",
    "NOTES: This is all pretty much just data preprocessing, getting the filenames that correspond to the subjects that we know we have EDSS scores for, then going and getting all of the data for each of those valid subjects. For each row of data per subject I add column (feature) that is the target feature, which is just their EDSS score for this time period. I then store that data in a list (called calid_subject_data) in order to facilitate creating the dataframe that I will use in the training/testing of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23510d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, it is the baseline of the gait data\n",
    "gait_baseline_filenames = glob.glob(\"data/Processed Data - MS +/Sway/MS1 Session 1/*\")\n",
    "#print((gait_baseline_filenames))\n",
    "removal = []\n",
    "for g in gait_baseline_filenames:\n",
    "    if g[-9:-4] not in valid_sids.keys():\n",
    "        removal.append(g)\n",
    "\n",
    "gait_b_filenames = [l for l in gait_baseline_filenames if l not in removal]\n",
    "\n",
    "###NOTE: In this test below, sometimes the two lists are not the same length\n",
    "# HOWEVER, the valid EDSS subject ids list is always longer, so we will always have a\n",
    "# \"target\" for each feature set, so we should be good to go\n",
    "#print(len(gait_b_filenames), len(valid_sids.keys()))\n",
    "\n",
    "\n",
    "# here, it is the 6mo of the gait data\n",
    "gait_6mo_filenames = glob.glob(\"data/Processed Data - MS +/Sway/MS1 Session 2/*\")\n",
    "#print((gait_baseline_filenames))\n",
    "removal = []\n",
    "for g in gait_baseline_filenames:\n",
    "    if g[-9:-4] not in valid_sids.keys():\n",
    "        removal.append(g)\n",
    "\n",
    "gait_6_filenames = [l for l in gait_baseline_filenames if l not in removal]\n",
    "\n",
    "# Now, loop through the valid files, get the features from each valid subject and assign\n",
    "MAX_ROWS_PER_SUBJECT = 20\n",
    "# their EDSS score as the \"target\"\n",
    "valid_subject_data = []\n",
    "cols = []\n",
    "for g in gait_b_filenames:\n",
    "    with open(g, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        if cols == []:\n",
    "            cols = next(file).strip().split(',')\n",
    "            cols.append('target')\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            if row[0] != 'timestamp_start':\n",
    "                row.append(valid_sids[g[-9:-4]][0])\n",
    "                valid_subject_data.append(row)\n",
    "                # if valid_sids[g[-9:-4]][0] == 0 and count < MAX_ROWS_PER_SUBJECT:\n",
    "                #     row.append(valid_sids[g[-9:-4]][0])\n",
    "                #     valid_subject_data.append(row)\n",
    "                #     count += 1\n",
    "                # elif valid_sids[g[-9:-4]][0] == 1:\n",
    "                #     row.append(valid_sids[g[-9:-4]][0])\n",
    "                #     valid_subject_data.append(row)\n",
    "                # else:\n",
    "                #     break\n",
    "\n",
    "# doing the exact some thing as before, just with the 6 month data\n",
    "# We can just add this data straight to the valid_subject_data list because it is all going\n",
    "# to be training data\n",
    "# We do have to separate the for loops though because we have to add the proper EDSS value\n",
    "# from the valid_sids dictionary\n",
    "for g6 in gait_6mo_filenames:\n",
    "    with open(g6, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        if cols == []:\n",
    "            cols = next(file).strip().split(',')\n",
    "            cols.append('target')\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            if row[0] != 'timestamp_start':\n",
    "                row.append(valid_sids[g[-9:-4]][1])\n",
    "                valid_subject_data.append(row)\n",
    "                # if valid_sids[g[-9:-4]][1] == 0 and count < MAX_ROWS_PER_SUBJECT:\n",
    "                #     row.append(valid_sids[g[-9:-4]][1])\n",
    "                #     valid_subject_data.append(row)\n",
    "                #     count += 1\n",
    "                # elif valid_sids[g[-9:-4]][1] == 1:\n",
    "                #     row.append(valid_sids[g[-9:-4]][1])\n",
    "                #     valid_subject_data.append(row)\n",
    "                # else:\n",
    "                #     break\n",
    "#print(cols)\n",
    "#print(valid_subject_data)\n",
    "num_observations = len(valid_subject_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0e020",
   "metadata": {},
   "source": [
    "Here, I will build a neural network and we will test how well that predicts the outputs compared to the random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d23ef6a",
   "metadata": {},
   "source": [
    "Final setup for the features dataframe and then training/testing the SVM model!\n",
    "\n",
    "NOTES:\n",
    "As you can see from the models that are commented out, I tried a number of different models, and it looks like the random forest classifier is going to be the one that works the best. Basically, here, I drop all of the non-important features colums, break the data in to testing and training partitions, train the model and then test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06358237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "predicting\n",
      "\n",
      "Printing confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.5</th>\n",
       "      <th>2.0</th>\n",
       "      <th>2.5</th>\n",
       "      <th>3.0</th>\n",
       "      <th>3.5</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <td>5</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.5</th>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1.0   1.5  2.0  2.5  3.0  3.5  4.0  5.0  6.0\n",
       "1.0    9   104    0    0    0    0    0    5    0\n",
       "1.5    5  1278    0    0    1    1    0    4    9\n",
       "2.0    0    55    1    0    0    0    0    0    0\n",
       "2.5    0    45    0    0    0    0    0    0    0\n",
       "3.0    1   137    0    0    2    0    0    0    0\n",
       "3.5    1   178    0    0    0    0    0    0    0\n",
       "4.0    0    52    0    0    0    0    0    0    1\n",
       "5.0    0    18    0    0    0    0    0   17    0\n",
       "6.0    0    71    0    0    0    0    0    0   18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(valid_subject_data, columns=cols)\n",
    "#print(df)\n",
    "#get rid of the non-important or NaN valued \"features\"\n",
    "df.drop(df.columns[[0,1,2,6,16]], axis=1, inplace=True)\n",
    "#print(df)\n",
    "df.fillna(0)\n",
    "\n",
    "#Train the model and see what happens!\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, df.columns != 'target'], np.array(df.iloc[:, df.columns == 'target']).reshape(num_observations,), test_size = 0.2, random_state = 0)\n",
    "#svm = SVC(kernel=\"poly\")\n",
    "#lin_model = linear_model.LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "print('training')\n",
    "rfc.fit(x_train, y_train)\n",
    "print(\"predicting\")\n",
    "svm_y_predict = rfc.predict(x_test)\n",
    "\n",
    "conf_matrix_svm = confusion_matrix(y_test, svm_y_predict)\n",
    "print(\"\\nPrinting confusion matrix\")\n",
    "ts = list(set(df['target']))\n",
    "ts = sorted([float(t) for t in ts])\n",
    "conf_matrix_to_df(conf_matrix_svm, ts)\n",
    "#print(conf_matrix_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdaf48",
   "metadata": {},
   "source": [
    "Notes moving forward to try and improve performance:\n",
    "\n",
    "1. Use a regression model rather than an SVM\n",
    "2. Do a better job of equalizing how much data we have from low EDSS scores (healthier people) vs high EDSS scores (not as healthy people)\n",
    "    - Currently, there is significantly more data from the healthier people, and not as much data from the not as healthy people, so all of the test data gets predicted as low EDSS (0). We can either omit a proportional amount of the low EDSS score training data, or we can add a bunch of mean-wise approximated data for high EDSS patients\n",
    "    - This second approach is not as scalable as the first because we can only add data based on data that we already have, so this approach would really only help us for the binary, low/high edss scores, any not the ultimate classification of individual EDSS score (we would then have a high density of data/scores for the small domain of high EDSS scores that we have recorded)\n",
    "\n",
    "Things done to address the problems/solutions above (3/22/21):\n",
    "1. Tried a regression model, worked worse than the SVM. Ended up with a RandomForestClassifier() that has proven to work pretty well, certainly much, much better than the SVM or the regression models (even though still not awesome)\n",
    "2. While it is not a perfect way of dealing with a disproportionate amount of data per target, I just limited the amount of data that there is in the processed dataset based on the target value. I limited healthier scores (target value 0) to 50 rows of data per subject, and did not limit the amount of data per target value 1 subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dffdfed",
   "metadata": {},
   "source": [
    "******* FINAL PROJECT WORK HERE *********<br>\n",
    "First attempt at cleaning the data that Nicole sent most recently with the summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "56bb2d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG DIFF: 6.83\n",
      "Error dispersion: Err < 5: 13   Err 5-10: 7    Err >10: 7\n"
     ]
    }
   ],
   "source": [
    "SWAY_1_DATA_FILENAME = \"data/summary_stats_data/sway1_stats.csv\"\n",
    "SWAY_2_DATA_FILENAME = \"data/summary_stats_data/sway2_stats.csv\"\n",
    "ABC_SCORES_1 = \"data/summary_stats_data/Scores_1.csv\"\n",
    "ABC_SCORES_2 = \"data/summary_stats_data/Scores_2.csv\"\n",
    "feature_names = [ 'subjectID', 'p5_RANGE', 'p25_RANGE', 'median_RANGE', 'p75_RANGE', 'p95_RANGE', 'p5_F50', 'p25_F50', 'median_F50', 'p75_F50', 'p95_F50', 'p5_F95', 'p25_F95', 'median_F95', 'p75_F95', 'p95_F95']\n",
    "\n",
    "# so, data is going to be a 2 dimensional list, where the value that is in each index of each 'row' corresponds to the positions of the labels in the list above\n",
    "# so the p5_RANGE value for subject 0 will be in data[0][0], p25_RANGE value for subject 0 will be in data[0][1], etc.\n",
    "# there aren't any labels, just gotta trust that they are correct. In my opinion this is easier than using a dictionary or something\n",
    "# because we can literally just pass this array straight to whatever model we want to use.\n",
    "\n",
    "####### SESSION 1 DATA COLLECTION SWAY DATA#########\n",
    "data_1 = []\n",
    "with open(SWAY_1_DATA_FILENAME, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        count = 0\n",
    "        col_nums = []\n",
    "        for r in reader:\n",
    "            if count == 0:\n",
    "                col_nums = [r.index(n) for n in r if n in feature_names]\n",
    "                count+=1\n",
    "            else:\n",
    "                data_row = [i for i in r if r.index(i) in col_nums]\n",
    "                s_id = data_row.pop()\n",
    "                data_row.insert(0, s_id)\n",
    "                data_1.append(data_row)\n",
    "data_1.pop()\n",
    "# next thing to do is to associate the ABC scores with the subject id's\n",
    "# i'm just going to do that randomly for now because I want to be able to test\n",
    "\n",
    "# now, with scores, keep track of ABC score per subjectID \n",
    "scores_1 = {}\n",
    "with open(ABC_SCORES_1, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        count = 0\n",
    "        col_num = -1\n",
    "        for r in reader:\n",
    "            scores_1[r[2]] = r[13]\n",
    "\n",
    "score_affiliated_data_1 = []\n",
    "x = 0\n",
    "while x < len(data_1):\n",
    "    data_1[x].append(scores_1[data_1[x][0]])\n",
    "    temp = data_1[x][1:]\n",
    "    score_affiliated_data_1.append(temp)\n",
    "    x += 1\n",
    "\n",
    "\n",
    "####### SESSION 2 DATA COLLECTION #########\n",
    "data_2 = []\n",
    "with open(SWAY_2_DATA_FILENAME, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        count = 0\n",
    "        col_nums = []\n",
    "        for r in reader:\n",
    "            if count == 0:\n",
    "                col_nums = [r.index(n) for n in r if n in feature_names]\n",
    "                count+=1\n",
    "            else:\n",
    "                data_row = [i for i in r if r.index(i) in col_nums]\n",
    "                s_id = data_row.pop()\n",
    "                data_row.insert(0, s_id)\n",
    "                data_2.append(data_row)\n",
    "data_2.pop()\n",
    "# next thing to do is to associate the ABC scores with the subject id's\n",
    "# i'm just going to do that randomly for now because I want to be able to test\n",
    "\n",
    "# now, with scores, keep track of ABC score per subjectID \n",
    "scores_2 = {}\n",
    "with open(ABC_SCORES_2, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        count = 0\n",
    "        col_num = -1\n",
    "        for r in reader:\n",
    "            if r[0] == \"\":\n",
    "                continue\n",
    "            else:\n",
    "                scores_2[r[2]] = r[11]\n",
    "\n",
    "score_affiliated_data_2 = []\n",
    "x = 0\n",
    "while x < len(data_2):\n",
    "    try:\n",
    "        data_2[x].append(scores_2[data_2[x][0]])\n",
    "        temp = data_2[x][1:]\n",
    "        score_affiliated_data_2.append(temp)\n",
    "        x += 1\n",
    "    except:\n",
    "        x += 1\n",
    "        continue\n",
    "\n",
    "# for s in score_affiliated_data:\n",
    "#     print(s)\n",
    "                \n",
    "\n",
    "### LOGISTIC REGRESSION ON SWAY DATA###\n",
    "x_train = [l[:-1] for l in score_affiliated_data_1]\n",
    "y_train = [l[-1] for l in score_affiliated_data_1]\n",
    "\n",
    "x_test = [l[:-1] for l in score_affiliated_data_2]\n",
    "y_test = [l[-1] for l in score_affiliated_data_2]\n",
    "\n",
    "\n",
    "#model = linear_model.LogisticRegression(max_iter=300)\n",
    "#model = linear_model.LinearRegression()\n",
    "model = SVC(C=6, kernel='linear')\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "tot_err = 0\n",
    "diffs = {\"<5\":0, \"5<10\":0, \">10\":0}\n",
    "for i in range(len(y_test)):\n",
    "    err = abs(float(y_pred[i])-float(y_test[i]))\n",
    "    tot_err += err\n",
    "    if err <= 5:\n",
    "        diffs[\"<5\"] += 1\n",
    "    elif err > 10:\n",
    "        diffs[\">10\"] += 1\n",
    "    else:\n",
    "        diffs[\"5<10\"] += 1\n",
    "    #print(f\"PRED: {y_pred[i]} ACTUAL: {y_test[i]}.....DIF: {err:.2f}\")\n",
    "print(f\"AVG DIFF: {tot_err/len(y_test):.2f}\")\n",
    "print(f\"Error dispersion: Err < 5: {diffs['<5']}   Err 5-10: {diffs['5<10']}    Err >10: {diffs['>10']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a369d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MORE REGRESSION MODEL OPTIONS ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5d2e9",
   "metadata": {},
   "source": [
    "Here, I will create a Neural Network classifier for ABC scores that are low, medium/low, medium, medium/high and high (based pretty much just on 20% intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71220f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 78.6228 - acc: 0.1481\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.6055 - acc: 0.4444\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.6019 - acc: 0.4444\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 0s 997us/step - loss: 1.5989 - acc: 0.4444\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5956 - acc: 0.4444\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5918 - acc: 0.4444\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5889 - acc: 0.4444\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5853 - acc: 0.4444\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5826 - acc: 0.4444\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5796 - acc: 0.4444\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5768 - acc: 0.4444\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5738 - acc: 0.4444\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5709 - acc: 0.4444\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5679 - acc: 0.4444\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5648 - acc: 0.4444\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021D6DD458B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "### DATA CLASSES CONSTRUCTION ###\n",
    "classes = {\"low\": 60, \"med_low\":70, \"med\":80, \"med_high\":90, \"high\":100}\n",
    "\n",
    "# training set\n",
    "class_affiliated_data_1 = []\n",
    "for s in score_affiliated_data_1:\n",
    "    classed_data = [float(k) for k in s[:15]]\n",
    "    if float(s[15]) < classes[\"low\"]:\n",
    "        classed_data.append(0)\n",
    "    elif float(s[15]) < classes[\"med_low\"]:\n",
    "        classed_data.append(1)\n",
    "    elif float(s[15]) < classes[\"med\"]:\n",
    "        classed_data.append(2)\n",
    "    elif float(s[15]) < classes[\"med_high\"]:\n",
    "        classed_data.append(3)\n",
    "    else:\n",
    "        classed_data.append(4)\n",
    "    class_affiliated_data_1.append(classed_data)\n",
    "\n",
    "# test set\n",
    "class_affiliated_data_2 = []\n",
    "for s in score_affiliated_data_2:\n",
    "    classed_data = [float(p) for p in s[:15]]\n",
    "    if float(s[15]) < classes[\"low\"]:\n",
    "        classed_data.append(0)\n",
    "    elif float(s[15]) < classes[\"med_low\"]:\n",
    "        classed_data.append(1)\n",
    "    elif float(s[15]) < classes[\"med\"]:\n",
    "        classed_data.append(2)\n",
    "    elif float(s[15]) < classes[\"med_high\"]:\n",
    "        classed_data.append(3)\n",
    "    else:\n",
    "        classed_data.append(4)\n",
    "    class_affiliated_data_2.append(classed_data)\n",
    "\n",
    "# for c in class_affiliated_data_1:\n",
    "#     print(c[15])\n",
    "\n",
    "x_nn_train = np.array([l[:-1] for l in class_affiliated_data_2])\n",
    "y_nn_train = [l[-1] for l in class_affiliated_data_2]\n",
    "y_nn_train_1hot = to_categorical(y_nn_train)\n",
    "\n",
    "x_nn_test = np.array([l[:-1] for l in class_affiliated_data_2])\n",
    "y_nn_test = [l[-1] for l in class_affiliated_data_2]\n",
    "y_nn_test_1hot = to_categorical(y_nn_test)\n",
    "\n",
    "### NEURAL NETWORK CONSTRUCTION ###\n",
    "neural = Sequential()\n",
    "neural.add(layers.Dense(50, activation='relu'))\n",
    "neural.add(layers.Dense(30, activation='relu'))\n",
    "neural.add(layers.Dense(40, activation='relu'))\n",
    "neural.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "neural.compile(optimizer='SGD', loss=\"categorical_crossentropy\", metrics=['acc'])\n",
    "\n",
    "neural.fit(x_nn_train, y_nn_train_1hot, batch_size=7, epochs=15)\n",
    "\n",
    "results = neural.predict(x_nn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "083eee21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n",
      "[0.19, 0.2, 0.2, 0.19, 0.23]\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    simplified = [float(f\"{h:.2f}\") for h in r]\n",
    "    print(simplified)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3b0c1df0451de2450861cf6e9b9534d3abd9bddf6f0adeac9edb86cb5227e66"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
